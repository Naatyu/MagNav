{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a79fd8-fd28-4a44-bfd2-fa94e64d020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split Done\n",
      "LSTM(\n",
      "  (lstm1): LSTM(25, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm2): LSTM(64, 32, num_layers=2, batch_first=True)\n",
      "  (lstm3): LSTM(32, 16, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                   | 0/8707 [00:00<?, ?batch/s]\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 182.81batch/s, lr=0.0002, train_loss=5.86e+4]\u001b[A\n",
      "Training:   0%|                    | 0/35 [00:48<?, ?epoch/s, lr=0.0002, train_loss=5.86e+4, val_loss=4.47e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 187.01batch/s, lr=0.0002, train_loss=5.02e+4]\u001b[A\n",
      "Training:   3%|▎            | 1/35 [01:36<27:12, 48.02s/epoch, lr=0.0002, train_loss=5.02e+4, val_loss=3.7e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 187.36batch/s, lr=0.0002, train_loss=4.3e+4]\u001b[A\n",
      "Training:   6%|▋            | 2/35 [02:24<26:33, 48.30s/epoch, lr=0.0002, train_loss=4.3e+4, val_loss=3.02e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 185.09batch/s, lr=0.0002, train_loss=3.69e+4]\u001b[A\n",
      "Training:   9%|█           | 3/35 [03:13<25:48, 48.38s/epoch, lr=0.0002, train_loss=3.69e+4, val_loss=2.48e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 188.73batch/s, lr=0.0002, train_loss=3.16e+4]\u001b[A\n",
      "Training:  11%|█▎          | 4/35 [04:01<25:01, 48.43s/epoch, lr=0.0002, train_loss=3.16e+4, val_loss=2.01e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 196.32batch/s, lr=0.0002, train_loss=2.7e+4]\u001b[A\n",
      "Training:  14%|█▊           | 5/35 [04:50<24:13, 48.44s/epoch, lr=0.0002, train_loss=2.7e+4, val_loss=1.67e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 190.85batch/s, lr=0.0002, train_loss=2.32e+4]\u001b[A\n",
      "Training:  17%|██          | 6/35 [05:38<23:22, 48.35s/epoch, lr=0.0002, train_loss=2.32e+4, val_loss=1.32e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 191.58batch/s, lr=0.0002, train_loss=1.98e+4]\u001b[A\n",
      "Training:  20%|██▍         | 7/35 [06:27<22:33, 48.35s/epoch, lr=0.0002, train_loss=1.98e+4, val_loss=1.07e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 190.84batch/s, lr=0.0002, train_loss=1.7e+4]\u001b[A\n",
      "Training:  23%|███▏          | 8/35 [07:15<21:47, 48.44s/epoch, lr=0.0002, train_loss=1.7e+4, val_loss=8.8e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 189.66batch/s, lr=0.0002, train_loss=1.46e+4]\u001b[A\n",
      "Training:  26%|███         | 9/35 [08:03<20:58, 48.39s/epoch, lr=0.0002, train_loss=1.46e+4, val_loss=6.93e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 197.36batch/s, lr=0.0002, train_loss=1.26e+4]\u001b[A\n",
      "Training:  29%|███▏       | 10/35 [08:52<20:08, 48.33s/epoch, lr=0.0002, train_loss=1.26e+4, val_loss=5.49e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 175.20batch/s, lr=0.0002, train_loss=1.09e+4]\u001b[A\n",
      "Training:  31%|███▍       | 11/35 [09:40<19:20, 48.36s/epoch, lr=0.0002, train_loss=1.09e+4, val_loss=4.62e+3]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:46<00:00, 176.99batch/s, lr=0.0002, train_loss=9.5e+3]\u001b[A\n",
      "Training:  34%|███▊       | 12/35 [10:28<18:31, 48.31s/epoch, lr=0.0002, train_loss=9.51e+3, val_loss=3.53e+3]\u001b[A\n",
      "Training:  69%|████████████████▌       | 5999/8707 [00:31<00:15, 178.80batch/s, lr=0.0002, train_loss=8.79e+3]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3761/1597911532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;31m#     Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3761/1597911532.py\u001b[0m in \u001b[0;36mmake_training\u001b[0;34m(model, EPOCHS)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# Adjust learning weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projet/venv/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projet/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|████████████████▌       | 6000/8707 [00:51<00:15, 178.80batch/s, lr=0.0002, train_loss=8.79e+3]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "#--- Functions ---#\n",
    "\n",
    "def trim_data(data,seq_length):\n",
    "    # Remove excessive data that cannot be in a full sequence\n",
    "    if (len(data)%seq_length) != 0:\n",
    "        data = data[:-(len(data)%seq_length)]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# class MagNavDataset(Dataset):\n",
    "#     # split can be 'Train', 'Val', 'Test'\n",
    "#     def __init__(self, df, seq_length, split):\n",
    "        \n",
    "#         self.seq_length = seq_length\n",
    "        \n",
    "#         # Get list of features\n",
    "#         self.features   = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "#         if split == 'train':\n",
    "            \n",
    "#             # Keeping only 1003, 1002, 1006 and 1004 flight sections for training except 1002.14\n",
    "#             sections = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "#             self.sections = sections\n",
    "            \n",
    "#             mask_train = pd.Series(dtype=bool)\n",
    "#             for line in sections:\n",
    "#                 mask  = (df.LINE == line)\n",
    "#                 mask_train = mask|mask_train\n",
    "            \n",
    "#             # Split in X, y for training\n",
    "#             X_train    = df.loc[mask_train,self.features]\n",
    "#             y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "#         elif split == 'val':\n",
    "            \n",
    "#             # Selecting 1007 for validation\n",
    "#             val_sections = df7.LINE.unique().tolist()\n",
    "#             val_sections.remove(1007.06)\n",
    "#             self.sections = val_sections\n",
    "            \n",
    "#             mask_val = pd.Series(dtype=bool)\n",
    "#             for line in val_sections:\n",
    "#                 mask  = (df.LINE == line)\n",
    "#                 mask_val = mask|mask_val\n",
    "            \n",
    "#             # Split in X, y for validation\n",
    "#             X_val      = df.loc[mask_val,self.features]\n",
    "#             y_val      = df.loc[mask_val,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_val.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_val.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "        \n",
    "#         elif split == 'test':\n",
    "            \n",
    "#             # Slecting flight 1007 as test\n",
    "#             mask_test = pd.Series(dtype=bool)\n",
    "# #             for line in df7.LINE.unique():\n",
    "#             mask_test  = (df.LINE == 1007.06)\n",
    "# #                 mask_test = mask|mask_test\n",
    "            \n",
    "#             # Split in X, y for test\n",
    "#             X_test     = df.loc[mask_test,self.features]\n",
    "#             y_test     = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         X = self.X[:,index:(index+self.seq_length)]\n",
    "#         y = self.y[index+self.seq_length-1]\n",
    "#         return X, y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(torch.t(self.X))-self.seq_length\n",
    "class MagNavDataset(Dataset):\n",
    "    # split can be 'Train', 'Val', 'Test'\n",
    "    def __init__(self, df, seq_length, n_fold, split):\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Get list of features\n",
    "        self.features = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "        # Get train sections for fold n\n",
    "        train_fold_0 = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "        test_fold_0  = df7.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_1 = np.concatenate([df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique()]).tolist()\n",
    "        test_fold_1  = df2.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_2 = np.concatenate([df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique(),df2.LINE.unique()]).tolist()\n",
    "        test_fold_2  = df3.LINE.unique().tolist()\n",
    "        \n",
    "        if n_fold == 0:\n",
    "            self.train_sections = train_fold_0\n",
    "            self.test_sections = test_fold_0\n",
    "        elif n_fold == 1:\n",
    "            self.train_sections = train_fold_1\n",
    "            self.test_sections = test_fold_1\n",
    "        elif n_fold == 2:\n",
    "            self.train_sections = train_fold_2\n",
    "            self.test_sections = test_fold_2\n",
    "        \n",
    "        \n",
    "        if split == 'train':\n",
    "            \n",
    "            mask_train = pd.Series(dtype=bool)\n",
    "            for line in self.train_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_train = mask|mask_train\n",
    "            \n",
    "            # Split in X, y for training\n",
    "            X_train    = df.loc[mask_train,self.features]\n",
    "            y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "        elif split == 'test':\n",
    "            \n",
    "            mask_test = pd.Series(dtype=bool)\n",
    "            for line in self.test_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_test = mask|mask_test\n",
    "            \n",
    "            # Split in X, y for test\n",
    "            X_test      = df.loc[mask_test,self.features]\n",
    "            y_test      = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[:,index:(index+self.seq_length)]\n",
    "        y = self.y[index+self.seq_length-1]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(torch.t(self.X))-self.seq_length\n",
    "\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(yhat,y)+1e-6)\n",
    "        return loss \n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(seq_len, 64, 1, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.lstm2 = torch.nn.LSTM(64, 32, 2, batch_first=True)\n",
    "        self.lstm3 = torch.nn.LSTM(32, 16, 2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(16,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = torch.zeros(1, x.size(0), 64).to('cuda')\n",
    "        c1 = torch.zeros(1, x.size(0), 64).to('cuda')\n",
    "        \n",
    "        h2 = torch.zeros(2, x.size(0), 32).to('cuda')\n",
    "        c2 = torch.zeros(2, x.size(0), 32).to('cuda')\n",
    "    \n",
    "        h3 = torch.zeros(2, x.size(0), 16).to('cuda')\n",
    "        c3 = torch.zeros(2, x.size(0), 16).to('cuda')\n",
    "        \n",
    "        out, hidden = self.lstm1(x, (h1,c1))\n",
    "        out = self.dropout(out)\n",
    "        out, hidden = self.lstm2(out, (h2,c2))\n",
    "        out, hidden = self.lstm3(out, (h3,c3))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to('cuda')\n",
    "        \n",
    "        out, _ = self.gru(x,h0)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class flex_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(flex_LSTM).__init__()\n",
    "        \n",
    "        self.lstms = torch.nn.ModuleList()\n",
    "        \n",
    "        for i,hidden_size in enumerate(hidden_sizes):\n",
    "            input_size = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            self.lstms.append(torch.nn.LSTM(input_size, hidden_size, 1))\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "def make_training(model,EPOCHS):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=2e-4) \n",
    "    \n",
    "    batch_bar = tqdm(total=len(train)//BATCH_SIZE,unit=\"batch\",desc='Training',leave=False)\n",
    "    epoch_bar = tqdm(total=EPOCHS,unit=\"epoch\",desc='Training')\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    pred_history = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        #---TRAIN---#\n",
    "\n",
    "        train_running_loss = 0.\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train()\n",
    "        \n",
    "        batch_bar.reset()\n",
    "        # Enumerate allow to track batch index and intra-epoch reporting \n",
    "        for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Make prediction for this batch\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            # Zero gradients for every batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            train_running_loss += loss.item()\n",
    "            \n",
    "            batch_bar.set_postfix(train_loss=train_running_loss/(batch_index+1),lr=optimizer.param_groups[0]['lr'])\n",
    "            batch_bar.update()\n",
    "\n",
    "        train_loss = train_running_loss / batch_index\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        #---VALIDATION---#\n",
    "\n",
    "        val_running_loss = 0.\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for batch_index, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                # Make prediction\n",
    "                predictions = model(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "                # Gather data and report\n",
    "                val_running_loss += loss.item()\n",
    "                preds.append(predictions.cpu())\n",
    "                \n",
    "            preds = np.concatenate(preds)\n",
    "            pred_history.append(preds)\n",
    "            val_loss = val_running_loss / batch_index\n",
    "            val_loss_history.append(val_loss)\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss,val_loss=val_loss,lr=optimizer.param_groups[0]['lr'])\n",
    "        epoch_bar.update()\n",
    "\n",
    "    return train_loss_history, val_loss_history, pred_history\n",
    "\n",
    "    \n",
    "#--- Main ---#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Remove UserWarnings from Python\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "    \n",
    "    # Reproducibility\n",
    "    torch.manual_seed(27)\n",
    "    random.seed(27)\n",
    "    np.random.seed(27)\n",
    "    \n",
    "    EPOCHS     = 35#args.epochs\n",
    "    BATCH_SIZE = 64#args.batch\n",
    "    DEVICE     = 'cuda'#args.device\n",
    "    SEQ_LEN    = 25#args.seq\n",
    "    \n",
    "    # Import Data\n",
    "    \n",
    "    df2 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1002')\n",
    "    df3 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1003')\n",
    "    df4 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1004')\n",
    "    df6 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1006')\n",
    "    df7 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1007')\n",
    "    \n",
    "    scaling = 'none'\n",
    "    \n",
    "                #     print('Data scaling Done\\n')\n",
    "    \n",
    "    # Train, Validation, Test set\n",
    "    df_concat = pd.concat([df2,df3,df4,df6,df7],ignore_index=True,axis=0)\n",
    "\n",
    "    train = MagNavDataset(df_concat,seq_length=SEQ_LEN, n_fold=0, split='train')\n",
    "    val   = MagNavDataset(df_concat,seq_length=SEQ_LEN, n_fold=0, split='test')\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader  = DataLoader(train,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True,\n",
    "                           num_workers=0,\n",
    "                           pin_memory=False)\n",
    "\n",
    "    val_loader    = DataLoader(val,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=False,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False)\n",
    "    \n",
    "    print('Data split Done')\n",
    "    \n",
    "    # Model\n",
    "#     model = LSTM(SEQ_LEN,16,3).to(DEVICE) #16,3 pas trop mal pour LSTM avec seq = 25\n",
    "#     model.name = 'LSTM'\n",
    "    \n",
    "    model = LSTM(SEQ_LEN).to(DEVICE)\n",
    "    model.name = 'LSTM'\n",
    "    print(model)\n",
    "    \n",
    "    # Loss\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "#     Training\n",
    "    train_loss_history, val_loss_history, pred_history = make_training(model,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7562b02-4b3a-4644-8e04-9a8ae573eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0136c-34f0-4360-9a5c-3eb70e78c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_history[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24642a1c-942d-4f6c-bc88-129ef3867f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a8121-5c11-4c5b-b203-a05eaf037bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "import numpy as np\n",
    "\n",
    "num_val = len(pred_history[0])\n",
    "frames = len(pred_history)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1,figsize=[20,5])\n",
    "line, = ax1.plot(np.linspace(0,num_val-1,num_val),pred_history[0])\n",
    "ax1.set_ylim([-600,600])\n",
    "ax1.grid()\n",
    "ax1.set_ylabel('[nT]')\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_title('RNN fitting to time series')\n",
    "\n",
    "def animate(i):\n",
    "    line.set_data(np.linspace(0,num_val-1,num_val),pred_history[i])\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=frames, interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bbddf-7de4-4250-aa55-bb7ac0aac58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01884fd3-1332-4488-a370-7db0f8fd9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "writegif = matplotlib.animation.PillowWriter(fps=8)\n",
    "ani.save('rnnanim.gif',writer=writegif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb5c1c-70ac-4665-b538-07f9a12e10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = MagNavDataset(df_concat,seq_length=25,split='test')\n",
    "test_loader    = DataLoader(test,\n",
    "                       batch_size=64,\n",
    "                       shuffle=False,\n",
    "                       num_workers=0,\n",
    "                       pin_memory=False)\n",
    "preds = []\n",
    "\n",
    "for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds.append(model(inputs.to('cuda')).cpu())\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e45ce-cce5-4792-ba6d-0124846fb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import magnav\n",
    "plt.figure(figsize=[12,10])\n",
    "plt.plot(preds,label='Prediction')\n",
    "plt.plot(test.y,label='Truth')\n",
    "RMSE = magnav.rmse(preds,test.y[25:],False)\n",
    "plt.text(0.005,0.975,f'RMSE={RMSE:.2f}nT',fontsize=12,bbox=dict(facecolor = 'C0',alpha=0.6),transform=plt.gca().transAxes)\n",
    "plt.grid()\n",
    "plt.title('Prediction for flight section 1007')\n",
    "plt.legend()\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('[nT]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eb9b4-5235-42b6-846f-da1c1fcb5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(preds-test.y[25:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0983833-3b6c-4fd9-9f2e-43a7e01aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizess = [100,50,50,25,25]\n",
    "input_size = 11\n",
    "for i,hidden_size in enumerate(hidden_sizes):\n",
    "    \n",
    "    input_size = input_size if i == 0 else hidden_sizes[i-1]\n",
    "    print(input_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce683a-9b89-4f3f-a0eb-523cf573faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,5):\n",
    "    if k == 3:\n",
    "        print(\"yo\")\n",
    "        continue\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592ea53-ce0a-4e73-b265-23f9a7fbf753",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "for k in range(num_layers):\n",
    "    if k == num_layers-1:\n",
    "        print('ouais')\n",
    "\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528cd95-ea90-4a65-964f-5eba72f6efc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12ec14-f6b6-44e0-9e94-810eb4952c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bb1eb-c563-4e53-a9c0-9218e0c232a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62d08c-22ce-47c1-b51e-7c822432c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, drop_lstm1, hidden_size, num_layers, num_LSTM, num_linear, num_neurons):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_LSTM = num_LSTM\n",
    "        self.num_linear = num_linear\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_lstm1 = drop_lstm1\n",
    "        self.num_layers = num_layers\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        \n",
    "        for k in range(num_LSTM):\n",
    "            if k == 0:\n",
    "                self.lstms.append(nn.LSTM(seq_len, hidden_size[0], num_layers[0], batch_first=True))\n",
    "                continue\n",
    "                \n",
    "            self.lstms.append(nn.LSTM(hidden_size[k-1], hidden_size[k], num_layers[k], batch_first=True))\n",
    "            \n",
    "        for n in range(num_linear):\n",
    "            if n == 0:\n",
    "                self.linears.append(nn.Linear(hidden_size[-1], num_neurons[0]))\n",
    "                continue\n",
    "            \n",
    "            self.linears.append(nn.Linear(num_neurons[n-1], num_neurons[n]))\n",
    "        \n",
    "        self.linears.append(nn.Linear(num_neurons[-1],1))\n",
    "            \n",
    "        for k in range(num_LSTM):\n",
    "            nn.init.kaiming_normal_(self.lstms[k]._parameters['weight_ih_l0'])\n",
    "            nn.init.kaiming_normal_(self.lstms[k]._parameters['weight_hh_l0'])\n",
    "            if self.lstms[k].bias is not None:\n",
    "                nn.init.constant_(self.lstms[k]._parameters['bias_ih_l0'], 0)\n",
    "                nn.init.constant_(self.lstms[k]._parameters['bias_hh_l0'], 0)\n",
    "        \n",
    "        for k in range(num_linear):\n",
    "            nn.init.kaiming_normal_(self.linears[k].weight)\n",
    "            if self.linears[k].bias is not None:\n",
    "                nn.init.constant_(self.linears[k].bias, 0)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for k, lstm_k in enumerate(self.lstms):\n",
    "            if k == 0:\n",
    "                h = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "                c = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "\n",
    "                out, _ = lstm_k(x, (h,c))\n",
    "                out = F.dropout(out)\n",
    "                continue\n",
    "                                   \n",
    "            h = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "            c = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "\n",
    "            out, _ = lstm_k(out, (h,c))\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        for k, linear_k in enumerate(self.linears):\n",
    "            if k == self.num_linear:\n",
    "                out = linear_k(out)\n",
    "                return out\n",
    "            \n",
    "            out = F.relu(linear_k(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aba87d-384e-4dc3-a986-7e2baa82ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_data(data,seq_length):\n",
    "    # Remove excessive data that cannot be in a full sequence\n",
    "    if (len(data)%seq_length) != 0:\n",
    "        data = data[:-(len(data)%seq_length)]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452e189-db84-4dbe-b1b6-36c51543c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagNavDataset(Dataset):\n",
    "    # split can be 'Train', 'Val', 'Test'\n",
    "    def __init__(self, df, seq_length, n_fold, split):\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Get list of features\n",
    "        self.features = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "        # Get train sections for fold n\n",
    "        train_fold_0 = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "        test_fold_0  = df7.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_1 = np.concatenate([df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique()]).tolist()\n",
    "        test_fold_1  = df2.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_2 = np.concatenate([df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique(),df2.LINE.unique()]).tolist()\n",
    "        test_fold_2  = df3.LINE.unique().tolist()\n",
    "        \n",
    "        if n_fold == 0:\n",
    "            self.train_sections = train_fold_0\n",
    "            self.test_sections = test_fold_0\n",
    "        elif n_fold == 1:\n",
    "            self.train_sections = train_fold_1\n",
    "            self.test_sections = test_fold_1\n",
    "        elif n_fold == 2:\n",
    "            self.train_sections = train_fold_2\n",
    "            self.test_sections = test_fold_2\n",
    "        \n",
    "        \n",
    "        if split == 'train':\n",
    "            \n",
    "            mask_train = pd.Series(dtype=bool)\n",
    "            for line in self.train_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_train = mask|mask_train\n",
    "            \n",
    "            # Split in X, y for training\n",
    "            X_train    = df.loc[mask_train,self.features]\n",
    "            y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "        elif split == 'test':\n",
    "            \n",
    "            mask_test = pd.Series(dtype=bool)\n",
    "            for line in self.test_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_test = mask|mask_test\n",
    "            \n",
    "            # Split in X, y for test\n",
    "            X_test      = df.loc[mask_test,self.features]\n",
    "            y_test      = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[:,index:(index+self.seq_length)]\n",
    "        y = self.y[index+self.seq_length-1]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(torch.t(self.X))-self.seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a729a-bd69-4077-b1ff-a0872cca97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer, seq_len, n_fold, batch_size):\n",
    "    \n",
    "    train_data = MagNavDataset(df_concat, seq_length=seq_len, n_fold=n_fold,split='train') # Train data\n",
    "    train_loader  = DataLoader(train_data,                                            # Train data loader\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False)\n",
    "    \n",
    "    network.train()                                                                   # Set the module in training mode\n",
    "    \n",
    "    for batch_i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "#         if batch_i * batch_size > number_of_train_examples:                     # Limit training data for faster computation\n",
    "#             break\n",
    "\n",
    "        optimizer.zero_grad()                                                         # Clear gradients\n",
    "        output = network(data.to(device))                                             # Forward propagration\n",
    "        loss = F.mse_loss(output, target.to(device))                                  # Compute loss (Mean Squared Error)\n",
    "        loss.backward()                                                               # Compute gradients\n",
    "        optimizer.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bd57-103d-41e4-8ea5-5924cc711333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network,seq_len, n_fold, batch_size):\n",
    "    \n",
    "    val_data   = MagNavDataset(df_concat, seq_length=seq_len, n_fold=n_fold,split='test') # Validation data\n",
    "    \n",
    "\n",
    "\n",
    "    val_loader    = DataLoader(val_data,                                              # Validation data loader\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False,\n",
    "                               drop_last=True)\n",
    "    \n",
    "    network.eval()                                                                    # Set module in evaluation mode\n",
    "    preds = []\n",
    "    truth = []\n",
    "    \n",
    "    with torch.no_grad():                                                             # Disable gradient calculation\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            \n",
    "#             if batch_i * batch_size > number_of_val_examples:                     # Limit validation data for faster computation\n",
    "#                 break\n",
    "            \n",
    "            preds.append(network(data.to(device)))                                    # Forward propagation\n",
    "            truth.append(target.to(device))                                           # Collecting truth data\n",
    "            \n",
    "    preds = torch.cat(preds,dim=1)                                                    # Unification of sequences\n",
    "    truth = torch.cat(truth,dim=1)\n",
    "    validation_RMSE = torch.sqrt(F.mse_loss(preds,truth))                             # Compute RMSE\n",
    "    validation_SNR = compute_SNR(truth.cpu().numpy(),preds.cpu().numpy())\n",
    "    \n",
    "    return validation_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d1201-ae39-4bea-b8b1-160d7c9359a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    num_LSTM    = 3                         # Number of LSTM layers\n",
    "    hidden_size = [8,68,16]         # Hidden size by lstm layers\n",
    "    num_layers  = [3,8,2]           # Layers by lstm layers\n",
    "    num_linear  = 3                         # Number of fully connected layers\n",
    "    num_neurons = [64,32,3]       # Number of neurons for the FC layers\n",
    "    drop_lstm1  = 0.3                           # Drop for 1st LSTM layer\n",
    "    seq_len     = 25              # Length of a sequence\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    model = LSTM(seq_len, drop_lstm1, hidden_size, num_layers, \n",
    "                 num_LSTM, num_linear, num_neurons).to(device)                        # Generate the model\n",
    "    \n",
    "    optimizer_name = (\"Adam\")  # Optimizers\n",
    "    lr             = 0.001                  # Learning rates\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)             # Optimizer set up\n",
    "    \n",
    "    df2 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1002')              # Import flight 1002\n",
    "    df3 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1003')              # Import flight 1003\n",
    "    df4 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1004')              # Import flight 1004\n",
    "    df6 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1006')              # Import flight 1006\n",
    "    df7 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1007')              # Import flight 1007\n",
    "    \n",
    "    df_concat = pd.concat([df2,df3,df4,df6,df7], ignore_index=True, axis=0)           # Concatenate data\n",
    "    \n",
    "#     n_epochs = trial.suggest_int(\"n_epochs\",2,50)\n",
    "#     batch_size = int(trial.suggest_discrete_uniform(\"batch_size\",32,2048,32))\n",
    "    n_epochs = 2\n",
    "    batch_size = 128\n",
    "    \n",
    "    fold_RMSE = []\n",
    "    \n",
    "    for n_fold in range(3):\n",
    "        model = LSTM(seq_len, drop_lstm1, hidden_size, num_layers, num_LSTM, num_linear, num_neurons).to(device) # Generate the model\n",
    "        print(model)\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)             # Optimizer set up\n",
    "        for epoch in range(n_epochs):\n",
    "            train(model, optimizer, seq_len, n_fold, batch_size)                         # Training of the model\n",
    "            RMSE = validate(model, seq_len, n_fold, batch_size)                           # Evaluate the model\n",
    "        \n",
    "        fold_RMSE.append(RMSE)\n",
    "        trial.report(RMSE, epoch)                                                     # Report values\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()                                     # Prune training (if it is not promising)\n",
    "            \n",
    "    total_RMSE = sum(fold_RMSE)/3\n",
    "    \n",
    "    print(total_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17705d44-aaab-4c10-815a-fd334152db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MagNavDataset(df_concat, seq_length=32, n_fold=1,split='test') # Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44286fa4-d77e-4a1a-b477-44655f39476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.__getitem__(0)[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72670c-7839-4815-97b7-b65ab67ed479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaed57-07dc-4905-9e34-0fefd5f40080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ae6c2-300a-47c4-8dc0-f8f7424df898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
