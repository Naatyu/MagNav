{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc116790-bf20-448d-a6b6-9d59e9fdc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import argparse\n",
    "import warnings\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad32e80-0511-4109-a1aa-fabad6358ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\\Documents\\GitHub\\MagNav\\notebooks\n",
      "A:\\Documents\\GitHub\\MagNav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Move from notebooks to MagNav\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b867785-c0e9-4376-bf48-b43cce43768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.magnav as magnav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a79fd8-fd28-4a44-bfd2-fa94e64d020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split Done\n",
      "LSTM(\n",
      "  (lstm1): LSTM(25, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (lstm2): LSTM(64, 32, num_layers=2, batch_first=True)\n",
      "  (lstm3): LSTM(32, 16, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                   | 0/8707 [00:00<?, ?batch/s]\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 182.81batch/s, lr=0.0002, train_loss=5.86e+4]\u001b[A\n",
      "Training:   0%|                    | 0/35 [00:48<?, ?epoch/s, lr=0.0002, train_loss=5.86e+4, val_loss=4.47e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 187.01batch/s, lr=0.0002, train_loss=5.02e+4]\u001b[A\n",
      "Training:   3%|▎            | 1/35 [01:36<27:12, 48.02s/epoch, lr=0.0002, train_loss=5.02e+4, val_loss=3.7e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 187.36batch/s, lr=0.0002, train_loss=4.3e+4]\u001b[A\n",
      "Training:   6%|▋            | 2/35 [02:24<26:33, 48.30s/epoch, lr=0.0002, train_loss=4.3e+4, val_loss=3.02e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 185.09batch/s, lr=0.0002, train_loss=3.69e+4]\u001b[A\n",
      "Training:   9%|█           | 3/35 [03:13<25:48, 48.38s/epoch, lr=0.0002, train_loss=3.69e+4, val_loss=2.48e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 188.73batch/s, lr=0.0002, train_loss=3.16e+4]\u001b[A\n",
      "Training:  11%|█▎          | 4/35 [04:01<25:01, 48.43s/epoch, lr=0.0002, train_loss=3.16e+4, val_loss=2.01e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 196.32batch/s, lr=0.0002, train_loss=2.7e+4]\u001b[A\n",
      "Training:  14%|█▊           | 5/35 [04:50<24:13, 48.44s/epoch, lr=0.0002, train_loss=2.7e+4, val_loss=1.67e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 190.85batch/s, lr=0.0002, train_loss=2.32e+4]\u001b[A\n",
      "Training:  17%|██          | 6/35 [05:38<23:22, 48.35s/epoch, lr=0.0002, train_loss=2.32e+4, val_loss=1.32e+4]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 191.58batch/s, lr=0.0002, train_loss=1.98e+4]\u001b[A\n",
      "Training:  20%|██▍         | 7/35 [06:27<22:33, 48.35s/epoch, lr=0.0002, train_loss=1.98e+4, val_loss=1.07e+4]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:45<00:00, 190.84batch/s, lr=0.0002, train_loss=1.7e+4]\u001b[A\n",
      "Training:  23%|███▏          | 8/35 [07:15<21:47, 48.44s/epoch, lr=0.0002, train_loss=1.7e+4, val_loss=8.8e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 189.66batch/s, lr=0.0002, train_loss=1.46e+4]\u001b[A\n",
      "Training:  26%|███         | 9/35 [08:03<20:58, 48.39s/epoch, lr=0.0002, train_loss=1.46e+4, val_loss=6.93e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:46<00:00, 197.36batch/s, lr=0.0002, train_loss=1.26e+4]\u001b[A\n",
      "Training:  29%|███▏       | 10/35 [08:52<20:08, 48.33s/epoch, lr=0.0002, train_loss=1.26e+4, val_loss=5.49e+3]\u001b[A\n",
      "Training: 100%|████████████████████████| 8707/8707 [00:45<00:00, 175.20batch/s, lr=0.0002, train_loss=1.09e+4]\u001b[A\n",
      "Training:  31%|███▍       | 11/35 [09:40<19:20, 48.36s/epoch, lr=0.0002, train_loss=1.09e+4, val_loss=4.62e+3]\u001b[A\n",
      "Training: 100%|█████████████████████████| 8707/8707 [00:46<00:00, 176.99batch/s, lr=0.0002, train_loss=9.5e+3]\u001b[A\n",
      "Training:  34%|███▊       | 12/35 [10:28<18:31, 48.31s/epoch, lr=0.0002, train_loss=9.51e+3, val_loss=3.53e+3]\u001b[A\n",
      "Training:  69%|████████████████▌       | 5999/8707 [00:31<00:15, 178.80batch/s, lr=0.0002, train_loss=8.79e+3]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3761/1597911532.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;31m#     Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3761/1597911532.py\u001b[0m in \u001b[0;36mmake_training\u001b[0;34m(model, EPOCHS)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;31m# Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# Adjust learning weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projet/venv/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projet/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  69%|████████████████▌       | 6000/8707 [00:51<00:15, 178.80batch/s, lr=0.0002, train_loss=8.79e+3]"
     ]
    }
   ],
   "source": [
    "\n",
    "#--- Functions ---#\n",
    "\n",
    "def trim_data(data,seq_length):\n",
    "    # Remove excessive data that cannot be in a full sequence\n",
    "    if (len(data)%seq_length) != 0:\n",
    "        data = data[:-(len(data)%seq_length)]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# class MagNavDataset(Dataset):\n",
    "#     # split can be 'Train', 'Val', 'Test'\n",
    "#     def __init__(self, df, seq_length, split):\n",
    "        \n",
    "#         self.seq_length = seq_length\n",
    "        \n",
    "#         # Get list of features\n",
    "#         self.features   = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "#         if split == 'train':\n",
    "            \n",
    "#             # Keeping only 1003, 1002, 1006 and 1004 flight sections for training except 1002.14\n",
    "#             sections = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "#             self.sections = sections\n",
    "            \n",
    "#             mask_train = pd.Series(dtype=bool)\n",
    "#             for line in sections:\n",
    "#                 mask  = (df.LINE == line)\n",
    "#                 mask_train = mask|mask_train\n",
    "            \n",
    "#             # Split in X, y for training\n",
    "#             X_train    = df.loc[mask_train,self.features]\n",
    "#             y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "#         elif split == 'val':\n",
    "            \n",
    "#             # Selecting 1007 for validation\n",
    "#             val_sections = df7.LINE.unique().tolist()\n",
    "#             val_sections.remove(1007.06)\n",
    "#             self.sections = val_sections\n",
    "            \n",
    "#             mask_val = pd.Series(dtype=bool)\n",
    "#             for line in val_sections:\n",
    "#                 mask  = (df.LINE == line)\n",
    "#                 mask_val = mask|mask_val\n",
    "            \n",
    "#             # Split in X, y for validation\n",
    "#             X_val      = df.loc[mask_val,self.features]\n",
    "#             y_val      = df.loc[mask_val,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_val.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_val.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "        \n",
    "#         elif split == 'test':\n",
    "            \n",
    "#             # Slecting flight 1007 as test\n",
    "#             mask_test = pd.Series(dtype=bool)\n",
    "# #             for line in df7.LINE.unique():\n",
    "#             mask_test  = (df.LINE == 1007.06)\n",
    "# #                 mask_test = mask|mask_test\n",
    "            \n",
    "#             # Split in X, y for test\n",
    "#             X_test     = df.loc[mask_test,self.features]\n",
    "#             y_test     = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "#             # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "#             self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "#             self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         X = self.X[:,index:(index+self.seq_length)]\n",
    "#         y = self.y[index+self.seq_length-1]\n",
    "#         return X, y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(torch.t(self.X))-self.seq_length\n",
    "class MagNavDataset(Dataset):\n",
    "    # split can be 'Train', 'Val', 'Test'\n",
    "    def __init__(self, df, seq_length, n_fold, split):\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Get list of features\n",
    "        self.features = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "        # Get train sections for fold n\n",
    "        train_fold_0 = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "        test_fold_0  = df7.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_1 = np.concatenate([df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique()]).tolist()\n",
    "        test_fold_1  = df2.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_2 = np.concatenate([df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique(),df2.LINE.unique()]).tolist()\n",
    "        test_fold_2  = df3.LINE.unique().tolist()\n",
    "        \n",
    "        if n_fold == 0:\n",
    "            self.train_sections = train_fold_0\n",
    "            self.test_sections = test_fold_0\n",
    "        elif n_fold == 1:\n",
    "            self.train_sections = train_fold_1\n",
    "            self.test_sections = test_fold_1\n",
    "        elif n_fold == 2:\n",
    "            self.train_sections = train_fold_2\n",
    "            self.test_sections = test_fold_2\n",
    "        \n",
    "        \n",
    "        if split == 'train':\n",
    "            \n",
    "            mask_train = pd.Series(dtype=bool)\n",
    "            for line in self.train_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_train = mask|mask_train\n",
    "            \n",
    "            # Split in X, y for training\n",
    "            X_train    = df.loc[mask_train,self.features]\n",
    "            y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "        elif split == 'test':\n",
    "            \n",
    "            mask_test = pd.Series(dtype=bool)\n",
    "            for line in self.test_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_test = mask|mask_test\n",
    "            \n",
    "            # Split in X, y for test\n",
    "            X_test      = df.loc[mask_test,self.features]\n",
    "            y_test      = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[:,index:(index+self.seq_length)]\n",
    "        y = self.y[index+self.seq_length-1]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(torch.t(self.X))-self.seq_length\n",
    "\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(yhat,y)+1e-6)\n",
    "        return loss \n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = torch.nn.LSTM(seq_len, 64, 1, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.lstm2 = torch.nn.LSTM(64, 32, 2, batch_first=True)\n",
    "        self.lstm3 = torch.nn.LSTM(32, 16, 2, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(16,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = torch.zeros(1, x.size(0), 64).to('cuda')\n",
    "        c1 = torch.zeros(1, x.size(0), 64).to('cuda')\n",
    "        \n",
    "        h2 = torch.zeros(2, x.size(0), 32).to('cuda')\n",
    "        c2 = torch.zeros(2, x.size(0), 32).to('cuda')\n",
    "    \n",
    "        h3 = torch.zeros(2, x.size(0), 16).to('cuda')\n",
    "        c3 = torch.zeros(2, x.size(0), 16).to('cuda')\n",
    "        \n",
    "        out, hidden = self.lstm1(x, (h1,c1))\n",
    "        out = self.dropout(out)\n",
    "        out, hidden = self.lstm2(out, (h2,c2))\n",
    "        out, hidden = self.lstm3(out, (h3,c3))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(GRU, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = torch.nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.fc = torch.nn.Linear(hidden_size,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to('cuda')\n",
    "        \n",
    "        out, _ = self.gru(x,h0)\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class flex_LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super(flex_LSTM).__init__()\n",
    "        \n",
    "        self.lstms = torch.nn.ModuleList()\n",
    "        \n",
    "        for i,hidden_size in enumerate(hidden_sizes):\n",
    "            input_size = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            self.lstms.append(torch.nn.LSTM(input_size, hidden_size, 1))\n",
    "        \n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "def make_training(model,EPOCHS):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=2e-4) \n",
    "    \n",
    "    batch_bar = tqdm(total=len(train)//BATCH_SIZE,unit=\"batch\",desc='Training',leave=False)\n",
    "    epoch_bar = tqdm(total=EPOCHS,unit=\"epoch\",desc='Training')\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    pred_history = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        #---TRAIN---#\n",
    "\n",
    "        train_running_loss = 0.\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train()\n",
    "        \n",
    "        batch_bar.reset()\n",
    "        # Enumerate allow to track batch index and intra-epoch reporting \n",
    "        for batch_index, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Make prediction for this batch\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            # Zero gradients for every batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            train_running_loss += loss.item()\n",
    "            \n",
    "            batch_bar.set_postfix(train_loss=train_running_loss/(batch_index+1),lr=optimizer.param_groups[0]['lr'])\n",
    "            batch_bar.update()\n",
    "\n",
    "        train_loss = train_running_loss / batch_index\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        #---VALIDATION---#\n",
    "\n",
    "        val_running_loss = 0.\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for batch_index, (inputs, labels) in enumerate(val_loader):\n",
    "\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "                # Make prediction\n",
    "                predictions = model(inputs)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(predictions, labels)\n",
    "\n",
    "                # Gather data and report\n",
    "                val_running_loss += loss.item()\n",
    "                preds.append(predictions.cpu())\n",
    "                \n",
    "            preds = np.concatenate(preds)\n",
    "            pred_history.append(preds)\n",
    "            val_loss = val_running_loss / batch_index\n",
    "            val_loss_history.append(val_loss)\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss,val_loss=val_loss,lr=optimizer.param_groups[0]['lr'])\n",
    "        epoch_bar.update()\n",
    "\n",
    "    return train_loss_history, val_loss_history, pred_history\n",
    "\n",
    "    \n",
    "#--- Main ---#\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Remove UserWarnings from Python\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "    \n",
    "    # Reproducibility\n",
    "    torch.manual_seed(27)\n",
    "    random.seed(27)\n",
    "    np.random.seed(27)\n",
    "    \n",
    "    EPOCHS     = 35#args.epochs\n",
    "    BATCH_SIZE = 64#args.batch\n",
    "    DEVICE     = 'cuda'#args.device\n",
    "    SEQ_LEN    = 25#args.seq\n",
    "    \n",
    "    # Import Data\n",
    "    \n",
    "    df2 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1002')\n",
    "    df3 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1003')\n",
    "    df4 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1004')\n",
    "    df6 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1006')\n",
    "    df7 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1007')\n",
    "    \n",
    "    scaling = 'none'\n",
    "    \n",
    "                #     print('Data scaling Done\\n')\n",
    "    \n",
    "    # Train, Validation, Test set\n",
    "    df_concat = pd.concat([df2,df3,df4,df6,df7],ignore_index=True,axis=0)\n",
    "\n",
    "    train = MagNavDataset(df_concat,seq_length=SEQ_LEN, n_fold=0, split='train')\n",
    "    val   = MagNavDataset(df_concat,seq_length=SEQ_LEN, n_fold=0, split='test')\n",
    "    \n",
    "    # Dataloaders\n",
    "    train_loader  = DataLoader(train,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=True,\n",
    "                           num_workers=0,\n",
    "                           pin_memory=False)\n",
    "\n",
    "    val_loader    = DataLoader(val,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=False,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False)\n",
    "    \n",
    "    print('Data split Done')\n",
    "    \n",
    "    # Model\n",
    "#     model = LSTM(SEQ_LEN,16,3).to(DEVICE) #16,3 pas trop mal pour LSTM avec seq = 25\n",
    "#     model.name = 'LSTM'\n",
    "    \n",
    "    model = LSTM(SEQ_LEN).to(DEVICE)\n",
    "    model.name = 'LSTM'\n",
    "    print(model)\n",
    "    \n",
    "    # Loss\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "#     Training\n",
    "    train_loss_history, val_loss_history, pred_history = make_training(model,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7562b02-4b3a-4644-8e04-9a8ae573eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(val_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0136c-34f0-4360-9a5c-3eb70e78c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pred_history[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24642a1c-942d-4f6c-bc88-129ef3867f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a8121-5c11-4c5b-b203-a05eaf037bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "import numpy as np\n",
    "\n",
    "num_val = len(pred_history[0])\n",
    "frames = len(pred_history)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1,figsize=[20,5])\n",
    "line, = ax1.plot(np.linspace(0,num_val-1,num_val),pred_history[0])\n",
    "ax1.set_ylim([-600,600])\n",
    "ax1.grid()\n",
    "ax1.set_ylabel('[nT]')\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_title('RNN fitting to time series')\n",
    "\n",
    "def animate(i):\n",
    "    line.set_data(np.linspace(0,num_val-1,num_val),pred_history[i])\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=frames, interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bbddf-7de4-4250-aa55-bb7ac0aac58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01884fd3-1332-4488-a370-7db0f8fd9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "writegif = matplotlib.animation.PillowWriter(fps=8)\n",
    "ani.save('rnnanim.gif',writer=writegif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb5c1c-70ac-4665-b538-07f9a12e10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test  = MagNavDataset(df_concat,seq_length=25,split='test')\n",
    "test_loader    = DataLoader(test,\n",
    "                       batch_size=64,\n",
    "                       shuffle=False,\n",
    "                       num_workers=0,\n",
    "                       pin_memory=False)\n",
    "preds = []\n",
    "\n",
    "for batch_index, (inputs, labels) in enumerate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds.append(model(inputs.to('cuda')).cpu())\n",
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e45ce-cce5-4792-ba6d-0124846fb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import magnav\n",
    "plt.figure(figsize=[12,10])\n",
    "plt.plot(preds,label='Prediction')\n",
    "plt.plot(test.y,label='Truth')\n",
    "RMSE = magnav.rmse(preds,test.y[25:],False)\n",
    "plt.text(0.005,0.975,f'RMSE={RMSE:.2f}nT',fontsize=12,bbox=dict(facecolor = 'C0',alpha=0.6),transform=plt.gca().transAxes)\n",
    "plt.grid()\n",
    "plt.title('Prediction for flight section 1007')\n",
    "plt.legend()\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('[nT]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7eb9b4-5235-42b6-846f-da1c1fcb5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(preds-test.y[25:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0983833-3b6c-4fd9-9f2e-43a7e01aeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizess = [100,50,50,25,25]\n",
    "input_size = 11\n",
    "for i,hidden_size in enumerate(hidden_sizes):\n",
    "    \n",
    "    input_size = input_size if i == 0 else hidden_sizes[i-1]\n",
    "    print(input_size,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce683a-9b89-4f3f-a0eb-523cf573faa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1,5):\n",
    "    if k == 3:\n",
    "        print(\"yo\")\n",
    "        continue\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592ea53-ce0a-4e73-b265-23f9a7fbf753",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "for k in range(num_layers):\n",
    "    if k == num_layers-1:\n",
    "        print('ouais')\n",
    "\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528cd95-ea90-4a65-964f-5eba72f6efc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12ec14-f6b6-44e0-9e94-810eb4952c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bb1eb-c563-4e53-a9c0-9218e0c232a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62d08c-22ce-47c1-b51e-7c822432c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, drop_lstm1, hidden_size, num_layers, num_LSTM, num_linear, num_neurons):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_LSTM = num_LSTM\n",
    "        self.num_linear = num_linear\n",
    "        self.hidden_size = hidden_size\n",
    "        self.drop_lstm1 = drop_lstm1\n",
    "        self.num_layers = num_layers\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.linears = nn.ModuleList()\n",
    "        \n",
    "        for k in range(num_LSTM):\n",
    "            if k == 0:\n",
    "                self.lstms.append(nn.LSTM(seq_len, hidden_size[0], num_layers[0], batch_first=True))\n",
    "                continue\n",
    "                \n",
    "            self.lstms.append(nn.LSTM(hidden_size[k-1], hidden_size[k], num_layers[k], batch_first=True))\n",
    "            \n",
    "        for n in range(num_linear):\n",
    "            if n == 0:\n",
    "                self.linears.append(nn.Linear(hidden_size[-1], num_neurons[0]))\n",
    "                continue\n",
    "            \n",
    "            self.linears.append(nn.Linear(num_neurons[n-1], num_neurons[n]))\n",
    "        \n",
    "        self.linears.append(nn.Linear(num_neurons[-1],1))\n",
    "            \n",
    "        for k in range(num_LSTM):\n",
    "            nn.init.kaiming_normal_(self.lstms[k]._parameters['weight_ih_l0'])\n",
    "            nn.init.kaiming_normal_(self.lstms[k]._parameters['weight_hh_l0'])\n",
    "            if self.lstms[k].bias is not None:\n",
    "                nn.init.constant_(self.lstms[k]._parameters['bias_ih_l0'], 0)\n",
    "                nn.init.constant_(self.lstms[k]._parameters['bias_hh_l0'], 0)\n",
    "        \n",
    "        for k in range(num_linear):\n",
    "            nn.init.kaiming_normal_(self.linears[k].weight)\n",
    "            if self.linears[k].bias is not None:\n",
    "                nn.init.constant_(self.linears[k].bias, 0)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for k, lstm_k in enumerate(self.lstms):\n",
    "            if k == 0:\n",
    "                h = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "                c = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "\n",
    "                out, _ = lstm_k(x, (h,c))\n",
    "                out = F.dropout(out)\n",
    "                continue\n",
    "                                   \n",
    "            h = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "            c = torch.zeros(self.num_layers[k], x.size(0), self.hidden_size[k]).to('cuda')\n",
    "\n",
    "            out, _ = lstm_k(out, (h,c))\n",
    "        \n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        for k, linear_k in enumerate(self.linears):\n",
    "            if k == self.num_linear:\n",
    "                out = linear_k(out)\n",
    "                return out\n",
    "            \n",
    "            out = F.relu(linear_k(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aba87d-384e-4dc3-a986-7e2baa82ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_data(data,seq_length):\n",
    "    # Remove excessive data that cannot be in a full sequence\n",
    "    if (len(data)%seq_length) != 0:\n",
    "        data = data[:-(len(data)%seq_length)]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a452e189-db84-4dbe-b1b6-36c51543c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagNavDataset(Dataset):\n",
    "    # split can be 'Train', 'Val', 'Test'\n",
    "    def __init__(self, df, seq_length, n_fold, split):\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Get list of features\n",
    "        self.features = df.drop(columns=['LINE','IGRFMAG1']).columns.to_list()\n",
    "        \n",
    "        # Get train sections for fold n\n",
    "        train_fold_0 = np.concatenate([df2.LINE.unique(),df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique()]).tolist()\n",
    "        test_fold_0  = df7.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_1 = np.concatenate([df3.LINE.unique(),df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique()]).tolist()\n",
    "        test_fold_1  = df2.LINE.unique().tolist()\n",
    "        \n",
    "        train_fold_2 = np.concatenate([df4.LINE.unique(),df6.LINE.unique(),df7.LINE.unique(),df2.LINE.unique()]).tolist()\n",
    "        test_fold_2  = df3.LINE.unique().tolist()\n",
    "        \n",
    "        if n_fold == 0:\n",
    "            self.train_sections = train_fold_0\n",
    "            self.test_sections = test_fold_0\n",
    "        elif n_fold == 1:\n",
    "            self.train_sections = train_fold_1\n",
    "            self.test_sections = test_fold_1\n",
    "        elif n_fold == 2:\n",
    "            self.train_sections = train_fold_2\n",
    "            self.test_sections = test_fold_2\n",
    "        \n",
    "        \n",
    "        if split == 'train':\n",
    "            \n",
    "            mask_train = pd.Series(dtype=bool)\n",
    "            for line in self.train_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_train = mask|mask_train\n",
    "            \n",
    "            # Split in X, y for training\n",
    "            X_train    = df.loc[mask_train,self.features]\n",
    "            y_train    = df.loc[mask_train,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_train.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_train.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "            \n",
    "        elif split == 'test':\n",
    "            \n",
    "            mask_test = pd.Series(dtype=bool)\n",
    "            for line in self.test_sections:\n",
    "                mask  = (df.LINE == line)\n",
    "                mask_test = mask|mask_test\n",
    "            \n",
    "            # Split in X, y for test\n",
    "            X_test      = df.loc[mask_test,self.features]\n",
    "            y_test      = df.loc[mask_test,'IGRFMAG1']\n",
    "            \n",
    "            # Removing data that can't fit in full sequence and convert it to torch tensor\n",
    "            self.X = torch.t(trim_data(torch.tensor(X_test.to_numpy(),dtype=torch.float32),seq_length))\n",
    "            self.y = trim_data(torch.tensor(np.reshape(y_test.to_numpy(),[-1,1]),dtype=torch.float32),seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[:,index:(index+self.seq_length)]\n",
    "        y = self.y[index+self.seq_length-1]\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(torch.t(self.X))-self.seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a729a-bd69-4077-b1ff-a0872cca97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer, seq_len, n_fold, batch_size):\n",
    "    \n",
    "    train_data = MagNavDataset(df_concat, seq_length=seq_len, n_fold=n_fold,split='train') # Train data\n",
    "    train_loader  = DataLoader(train_data,                                            # Train data loader\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False)\n",
    "    \n",
    "    network.train()                                                                   # Set the module in training mode\n",
    "    \n",
    "    for batch_i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "#         if batch_i * batch_size > number_of_train_examples:                     # Limit training data for faster computation\n",
    "#             break\n",
    "\n",
    "        optimizer.zero_grad()                                                         # Clear gradients\n",
    "        output = network(data.to(device))                                             # Forward propagration\n",
    "        loss = F.mse_loss(output, target.to(device))                                  # Compute loss (Mean Squared Error)\n",
    "        loss.backward()                                                               # Compute gradients\n",
    "        optimizer.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2bd57-103d-41e4-8ea5-5924cc711333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(network,seq_len, n_fold, batch_size):\n",
    "    \n",
    "    val_data   = MagNavDataset(df_concat, seq_length=seq_len, n_fold=n_fold,split='test') # Validation data\n",
    "    \n",
    "\n",
    "\n",
    "    val_loader    = DataLoader(val_data,                                              # Validation data loader\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False,\n",
    "                               num_workers=0,\n",
    "                               pin_memory=False,\n",
    "                               drop_last=True)\n",
    "    \n",
    "    network.eval()                                                                    # Set module in evaluation mode\n",
    "    preds = []\n",
    "    truth = []\n",
    "    \n",
    "    with torch.no_grad():                                                             # Disable gradient calculation\n",
    "        for batch_i, (data, target) in enumerate(val_loader):\n",
    "            \n",
    "#             if batch_i * batch_size > number_of_val_examples:                     # Limit validation data for faster computation\n",
    "#                 break\n",
    "            \n",
    "            preds.append(network(data.to(device)))                                    # Forward propagation\n",
    "            truth.append(target.to(device))                                           # Collecting truth data\n",
    "            \n",
    "    preds = torch.cat(preds,dim=1)                                                    # Unification of sequences\n",
    "    truth = torch.cat(truth,dim=1)\n",
    "    validation_RMSE = torch.sqrt(F.mse_loss(preds,truth))                             # Compute RMSE\n",
    "    validation_SNR = compute_SNR(truth.cpu().numpy(),preds.cpu().numpy())\n",
    "    \n",
    "    return validation_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80d1201-ae39-4bea-b8b1-160d7c9359a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3117/783456606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m model = LSTM(seq_len, drop_lstm1, hidden_size, num_layers, \n\u001b[0m\u001b[1;32m     11\u001b[0m              num_LSTM, num_linear, num_neurons).to(device)                        # Generate the model\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "    num_LSTM    = 3                         # Number of LSTM layers\n",
    "    hidden_size = [8,68,16]         # Hidden size by lstm layers\n",
    "    num_layers  = [3,8,2]           # Layers by lstm layers\n",
    "    num_linear  = 3                         # Number of fully connected layers\n",
    "    num_neurons = [64,32,3]       # Number of neurons for the FC layers\n",
    "    drop_lstm1  = 0.3                           # Drop for 1st LSTM layer\n",
    "    seq_len     = 25              # Length of a sequence\n",
    "    device = \"cuda\"\n",
    "    \n",
    "    model = LSTM(seq_len, drop_lstm1, hidden_size, num_layers, \n",
    "                 num_LSTM, num_linear, num_neurons).to(device)                        # Generate the model\n",
    "    \n",
    "    optimizer_name = (\"Adam\")  # Optimizers\n",
    "    lr             = 0.001                  # Learning rates\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)             # Optimizer set up\n",
    "    \n",
    "    df2 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1002')              # Import flight 1002\n",
    "    df3 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1003')              # Import flight 1003\n",
    "    df4 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1004')              # Import flight 1004\n",
    "    df6 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1006')              # Import flight 1006\n",
    "    df7 = pd.read_hdf('./data/processed/Chall_dataset.h5', key=f'Flt1007')              # Import flight 1007\n",
    "    \n",
    "    df_concat = pd.concat([df2,df3,df4,df6,df7], ignore_index=True, axis=0)           # Concatenate data\n",
    "    \n",
    "#     n_epochs = trial.suggest_int(\"n_epochs\",2,50)\n",
    "#     batch_size = int(trial.suggest_discrete_uniform(\"batch_size\",32,2048,32))\n",
    "    n_epochs = 2\n",
    "    batch_size = 128\n",
    "    \n",
    "    fold_RMSE = []\n",
    "    \n",
    "    for n_fold in range(3):\n",
    "        model = LSTM(seq_len, drop_lstm1, hidden_size, num_layers, num_LSTM, num_linear, num_neurons).to(device) # Generate the model\n",
    "        print(model)\n",
    "        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)             # Optimizer set up\n",
    "        for epoch in range(n_epochs):\n",
    "            train(model, optimizer, seq_len, n_fold, batch_size)                         # Training of the model\n",
    "            RMSE = validate(model, seq_len, n_fold, batch_size)                           # Evaluate the model\n",
    "        \n",
    "        fold_RMSE.append(RMSE)\n",
    "        trial.report(RMSE, epoch)                                                     # Report values\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()                                     # Prune training (if it is not promising)\n",
    "            \n",
    "    total_RMSE = sum(fold_RMSE)/3\n",
    "    \n",
    "    print(total_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17705d44-aaab-4c10-815a-fd334152db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MagNavDataset(df_concat, seq_length=32, n_fold=1,split='test') # Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44286fa4-d77e-4a1a-b477-44655f39476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.__getitem__(0)[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72670c-7839-4815-97b7-b65ab67ed479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eeaed57-07dc-4905-9e34-0fefd5f40080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1002')              # Import flight 1002\n",
    "df3 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1003')              # Import flight 1003\n",
    "df4 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1004')              # Import flight 1004\n",
    "df6 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1006')              # Import flight 1006\n",
    "df7 = pd.read_hdf('../data/processed/Chall_dataset.h5', key=f'Flt1007')              # Import flight 1007\n",
    "\n",
    "df_concat = pd.concat([df2,df3,df4,df6,df7], ignore_index=True, axis=0)           # Concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515a945a-9238-4abb-97c1-54fc8b7154dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TL_comp_mag3_cl</th>\n",
       "      <th>TL_comp_mag5_cl</th>\n",
       "      <th>V_BAT1</th>\n",
       "      <th>V_BAT2</th>\n",
       "      <th>INS_ACC_X</th>\n",
       "      <th>INS_ACC_Y</th>\n",
       "      <th>INS_ACC_Z</th>\n",
       "      <th>CUR_IHTR</th>\n",
       "      <th>PITCH</th>\n",
       "      <th>ROLL</th>\n",
       "      <th>AZIMUTH</th>\n",
       "      <th>LINE</th>\n",
       "      <th>IGRFMAG1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-866.739462</td>\n",
       "      <td>149.998320</td>\n",
       "      <td>25.722757</td>\n",
       "      <td>2.656956</td>\n",
       "      <td>-0.007116</td>\n",
       "      <td>-0.001967</td>\n",
       "      <td>9.801125</td>\n",
       "      <td>4.279897</td>\n",
       "      <td>3.209615</td>\n",
       "      <td>0.680420</td>\n",
       "      <td>195.979442</td>\n",
       "      <td>1216.832183</td>\n",
       "      <td>-20.614485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>357.954672</td>\n",
       "      <td>256.205439</td>\n",
       "      <td>0.207510</td>\n",
       "      <td>0.771180</td>\n",
       "      <td>0.887870</td>\n",
       "      <td>0.869604</td>\n",
       "      <td>0.783899</td>\n",
       "      <td>1.293773</td>\n",
       "      <td>2.079876</td>\n",
       "      <td>6.923212</td>\n",
       "      <td>102.015984</td>\n",
       "      <td>802.839799</td>\n",
       "      <td>248.690317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-14799.947733</td>\n",
       "      <td>-1131.669624</td>\n",
       "      <td>25.565000</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>-7.366149</td>\n",
       "      <td>-6.812820</td>\n",
       "      <td>1.656503</td>\n",
       "      <td>1.197000</td>\n",
       "      <td>-9.240000</td>\n",
       "      <td>-33.980000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>-868.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1083.094048</td>\n",
       "      <td>11.551914</td>\n",
       "      <td>25.611000</td>\n",
       "      <td>2.131000</td>\n",
       "      <td>-0.218880</td>\n",
       "      <td>-0.189277</td>\n",
       "      <td>9.492046</td>\n",
       "      <td>3.279000</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>-1.210000</td>\n",
       "      <td>118.180000</td>\n",
       "      <td>1002.180000</td>\n",
       "      <td>-163.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-848.461127</td>\n",
       "      <td>167.309176</td>\n",
       "      <td>25.721000</td>\n",
       "      <td>2.965000</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>-0.000385</td>\n",
       "      <td>9.801126</td>\n",
       "      <td>4.101000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>181.840000</td>\n",
       "      <td>1003.080000</td>\n",
       "      <td>14.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-656.548771</td>\n",
       "      <td>289.203195</td>\n",
       "      <td>25.769000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>0.218683</td>\n",
       "      <td>0.193209</td>\n",
       "      <td>10.119414</td>\n",
       "      <td>5.192000</td>\n",
       "      <td>4.170000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>295.730000</td>\n",
       "      <td>1007.020000</td>\n",
       "      <td>106.659250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7499.829375</td>\n",
       "      <td>2878.672361</td>\n",
       "      <td>27.906000</td>\n",
       "      <td>3.688000</td>\n",
       "      <td>6.089665</td>\n",
       "      <td>7.133355</td>\n",
       "      <td>17.052216</td>\n",
       "      <td>6.670000</td>\n",
       "      <td>11.410000</td>\n",
       "      <td>42.220000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>4019.000000</td>\n",
       "      <td>2699.331000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TL_comp_mag3_cl  TL_comp_mag5_cl         V_BAT1         V_BAT2  \\\n",
       "count    671840.000000    671840.000000  671840.000000  671840.000000   \n",
       "mean       -866.739462       149.998320      25.722757       2.656956   \n",
       "std         357.954672       256.205439       0.207510       0.771180   \n",
       "min      -14799.947733     -1131.669624      25.565000       0.721000   \n",
       "25%       -1083.094048        11.551914      25.611000       2.131000   \n",
       "50%        -848.461127       167.309176      25.721000       2.965000   \n",
       "75%        -656.548771       289.203195      25.769000       3.190000   \n",
       "max        7499.829375      2878.672361      27.906000       3.688000   \n",
       "\n",
       "           INS_ACC_X      INS_ACC_Y      INS_ACC_Z       CUR_IHTR  \\\n",
       "count  671840.000000  671840.000000  671840.000000  671840.000000   \n",
       "mean       -0.007116      -0.001967       9.801125       4.279897   \n",
       "std         0.887870       0.869604       0.783899       1.293773   \n",
       "min        -7.366149      -6.812820       1.656503       1.197000   \n",
       "25%        -0.218880      -0.189277       9.492046       3.279000   \n",
       "50%         0.000177      -0.000385       9.801126       4.101000   \n",
       "75%         0.218683       0.193209      10.119414       5.192000   \n",
       "max         6.089665       7.133355      17.052216       6.670000   \n",
       "\n",
       "               PITCH           ROLL        AZIMUTH           LINE  \\\n",
       "count  671840.000000  671840.000000  671840.000000  671840.000000   \n",
       "mean        3.209615       0.680420     195.979442    1216.832183   \n",
       "std         2.079876       6.923212     102.015984     802.839799   \n",
       "min        -9.240000     -33.980000       0.000000     158.000000   \n",
       "25%         2.150000      -1.210000     118.180000    1002.180000   \n",
       "50%         3.170000       0.260000     181.840000    1003.080000   \n",
       "75%         4.170000       1.770000     295.730000    1007.020000   \n",
       "max        11.410000      42.220000     360.000000    4019.000000   \n",
       "\n",
       "            IGRFMAG1  \n",
       "count  671840.000000  \n",
       "mean      -20.614485  \n",
       "std       248.690317  \n",
       "min      -868.652000  \n",
       "25%      -163.520000  \n",
       "50%        14.248000  \n",
       "75%       106.659250  \n",
       "max      2699.331000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3d8559-4c9b-4c35-af89-f2eb50ed43fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TL_comp_mag3_cl</th>\n",
       "      <th>TL_comp_mag5_cl</th>\n",
       "      <th>V_BAT1</th>\n",
       "      <th>V_BAT2</th>\n",
       "      <th>INS_ACC_X</th>\n",
       "      <th>INS_ACC_Y</th>\n",
       "      <th>INS_ACC_Z</th>\n",
       "      <th>CUR_IHTR</th>\n",
       "      <th>PITCH</th>\n",
       "      <th>ROLL</th>\n",
       "      <th>AZIMUTH</th>\n",
       "      <th>LINE</th>\n",
       "      <th>IGRFMAG1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "      <td>671840.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.624814</td>\n",
       "      <td>0.319591</td>\n",
       "      <td>0.067389</td>\n",
       "      <td>0.652496</td>\n",
       "      <td>0.546904</td>\n",
       "      <td>0.488367</td>\n",
       "      <td>0.529019</td>\n",
       "      <td>0.563292</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.454861</td>\n",
       "      <td>0.544387</td>\n",
       "      <td>0.274238</td>\n",
       "      <td>0.237680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.063886</td>\n",
       "      <td>0.088641</td>\n",
       "      <td>0.259919</td>\n",
       "      <td>0.065984</td>\n",
       "      <td>0.062354</td>\n",
       "      <td>0.050917</td>\n",
       "      <td>0.236392</td>\n",
       "      <td>0.100720</td>\n",
       "      <td>0.090856</td>\n",
       "      <td>0.283378</td>\n",
       "      <td>0.207936</td>\n",
       "      <td>0.069701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.615112</td>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.019650</td>\n",
       "      <td>0.475228</td>\n",
       "      <td>0.531166</td>\n",
       "      <td>0.474936</td>\n",
       "      <td>0.508943</td>\n",
       "      <td>0.380413</td>\n",
       "      <td>0.551574</td>\n",
       "      <td>0.430052</td>\n",
       "      <td>0.328278</td>\n",
       "      <td>0.218643</td>\n",
       "      <td>0.197628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.323907</td>\n",
       "      <td>0.066638</td>\n",
       "      <td>0.756320</td>\n",
       "      <td>0.547446</td>\n",
       "      <td>0.488481</td>\n",
       "      <td>0.529019</td>\n",
       "      <td>0.530605</td>\n",
       "      <td>0.600969</td>\n",
       "      <td>0.449344</td>\n",
       "      <td>0.505111</td>\n",
       "      <td>0.218876</td>\n",
       "      <td>0.247451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.634239</td>\n",
       "      <td>0.354302</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>0.832154</td>\n",
       "      <td>0.563684</td>\n",
       "      <td>0.502362</td>\n",
       "      <td>0.549693</td>\n",
       "      <td>0.729947</td>\n",
       "      <td>0.649395</td>\n",
       "      <td>0.469160</td>\n",
       "      <td>0.821472</td>\n",
       "      <td>0.219896</td>\n",
       "      <td>0.273351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TL_comp_mag3_cl  TL_comp_mag5_cl         V_BAT1         V_BAT2  \\\n",
       "count    671840.000000    671840.000000  671840.000000  671840.000000   \n",
       "mean          0.624814         0.319591       0.067389       0.652496   \n",
       "std           0.016052         0.063886       0.088641       0.259919   \n",
       "min           0.000000         0.000000       0.000000       0.000000   \n",
       "25%           0.615112         0.285068       0.019650       0.475228   \n",
       "50%           0.625633         0.323907       0.066638       0.756320   \n",
       "75%           0.634239         0.354302       0.087142       0.832154   \n",
       "max           1.000000         1.000000       1.000000       1.000000   \n",
       "\n",
       "           INS_ACC_X      INS_ACC_Y      INS_ACC_Z       CUR_IHTR  \\\n",
       "count  671840.000000  671840.000000  671840.000000  671840.000000   \n",
       "mean        0.546904       0.488367       0.529019       0.563292   \n",
       "std         0.065984       0.062354       0.050917       0.236392   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.531166       0.474936       0.508943       0.380413   \n",
       "50%         0.547446       0.488481       0.529019       0.530605   \n",
       "75%         0.563684       0.502362       0.549693       0.729947   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "               PITCH           ROLL        AZIMUTH           LINE  \\\n",
       "count  671840.000000  671840.000000  671840.000000  671840.000000   \n",
       "mean        0.602887       0.454861       0.544387       0.274238   \n",
       "std         0.100720       0.090856       0.283378       0.207936   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.551574       0.430052       0.328278       0.218643   \n",
       "50%         0.600969       0.449344       0.505111       0.218876   \n",
       "75%         0.649395       0.469160       0.821472       0.219896   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "            IGRFMAG1  \n",
       "count  671840.000000  \n",
       "mean        0.237680  \n",
       "std         0.069701  \n",
       "min         0.000000  \n",
       "25%         0.197628  \n",
       "50%         0.247451  \n",
       "75%         0.273351  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound=[0,1]\n",
    "minmax_df = bound[0] + ((bound[1]-bound[0])*(df_concat-df_concat.min())/(df_concat.max()-df_concat.min()))\n",
    "minmax_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c4444b-b954-4e8f-84d4-19058f936a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_df['IGRFMAG1'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d44bb0c-7ae1-4a37-989a-e918bc537932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_df['INS_ACC_Y'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b5d8a19-84d7-4186-9eb8-c012424e1ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nlaoue/projet/MagNav/notebooks\n",
      "/home/nlaoue/projet/MagNav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Move from notebooks to MagNav\n",
    "print(os.getcwd())\n",
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242af3cb-a005-433e-bbfa-849e229f38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.magnav as magnav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdab7d6d-0087-4720-ba7c-0458452595b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.61645675765092 62.25599064075581\n"
     ]
    }
   ],
   "source": [
    "RMSEno = magnav.rmse(df_concat['TL_comp_mag5_cl'],df_concat['IGRFMAG1'],False)\n",
    "RMSEyes = magnav.rmse((norm_df['TL_comp_mag5_cl']*df_concat['TL_comp_mag5_cl'].std())+df_concat['TL_comp_mag5_cl'].mean(),(norm_df['IGRFMAG1']*df_concat['IGRFMAG1'].std())+df_concat['IGRFMAG1'].mean(),True)\n",
    "print(RMSEno,RMSEyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070fed-e9b3-43d9-a4a8-0d9bd8516527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b87c3fd-68de-49f2-a5a9-7eaa0c2ef086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(r'data/interim/Flt_data.h5', key=f'Flt100{3}')\n",
    "\n",
    "\n",
    "#---Tolles-Lawson compensation---#\n",
    "\n",
    "# Get cloverleaf pattern data\n",
    "flight_number = 2\n",
    "df_pattern    = pd.read_hdf(r'data/interim/Flt_data.h5',key=f'Flt100{flight_number}')\n",
    "mask  = (df_pattern.LINE == 1002.20)\n",
    "tl_cl = df_pattern[mask] # Square Tolles-Lawson pattern\n",
    "\n",
    "# filter parameters\n",
    "fs      = 10.0\n",
    "lowcut  = 0.1\n",
    "highcut = 0.9\n",
    "filt    = ['Butterworth',4]\n",
    "\n",
    "# A matrix of Tolles-Lawson\n",
    "A = magnav.create_TL_A(df['FLUXB_X'],df['FLUXB_Y'],df['FLUXB_Z'])\n",
    "\n",
    "# Tolles Lawson coefficients computation\n",
    "TL_coef_3 = magnav.create_TL_coef(tl_cl['FLUXB_X'],tl_cl['FLUXB_Y'],tl_cl['FLUXB_Z'],tl_cl['UNCOMPMAG3'],\n",
    "                              lowcut=lowcut,highcut=highcut,fs=fs,filter_params=filt)\n",
    "TL_coef_5 = magnav.create_TL_coef(tl_cl['FLUXB_X'],tl_cl['FLUXB_Y'],tl_cl['FLUXB_Z'],tl_cl['UNCOMPMAG5'],\n",
    "                              lowcut=lowcut,highcut=highcut,fs=fs,filter_params=filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a12331e-48d1-44e4-99c0-df04ed0117ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnetometers correction\n",
    "corr_mag3 = magnav.apply_TL(np.reshape(df['UNCOMPMAG3'].tolist(),(-1,1)),TL_coef_3,A)\n",
    "df['TL_comp_mag3_cl'] = np.reshape(df['UNCOMPMAG3'].tolist(),(-1,1))-np.dot(A,TL_coef_3)+np.mean(np.dot(A,TL_coef_3))\n",
    "df['TL_comp_mag5_cl'] = np.reshape(df['UNCOMPMAG5'].tolist(),(-1,1))-np.dot(A,TL_coef_5)+np.mean(np.dot(A,TL_coef_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad683b-aac7-4a8a-826d-891d544ec2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed40b01-cf5a-4334-94ec-b9dcf328e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df2 = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt1002')\n",
    "    df3 = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt1003')\n",
    "    df4 = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt1004')\n",
    "    df6 = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt1006')\n",
    "    df7 = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt1007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "900827a3-4dd8-4f4a-994a-f04a321a2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_dict = {}\n",
    "for n in [2,3,4,6,7]:\n",
    "    df = pd.read_hdf('./data/processed/Flt_data.h5', key=f'Flt100{n}')\n",
    "    flights_dict[n] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bf90f05-dd09-4779-90bd-19be61697845",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = flights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da371c9-a5a9-408b-b970-22f9f4aaa639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
